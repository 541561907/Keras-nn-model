{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "loading vocab and processing data\n",
      "loading vocab and processing data\n",
      "(19997, 'train sequences')\n",
      "(104, 'test sequences')\n",
      "Pad sequences (samples x time)\n",
      "('x_train shape:', (19997, 500))\n",
      "('x_test shape:', (104, 500))\n",
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/150\n",
      "19997/19997 [==============================] - 240s - loss: 0.2090 - f1score: 0.0451   \n",
      "Epoch 2/150\n",
      "19997/19997 [==============================] - 248s - loss: 0.1723 - f1score: 0.1065   \n",
      "Epoch 3/150\n",
      "19997/19997 [==============================] - 332s - loss: 0.1520 - f1score: 0.1981   \n",
      "Epoch 4/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.1462 - f1score: 0.2196   \n",
      "Epoch 5/150\n",
      "19997/19997 [==============================] - 225s - loss: 0.1391 - f1score: 0.2412   \n",
      "Epoch 6/150\n",
      "19997/19997 [==============================] - 229s - loss: 0.1409 - f1score: 0.2450   \n",
      "Epoch 7/150\n",
      "19997/19997 [==============================] - 225s - loss: 0.1350 - f1score: 0.2640   \n",
      "Epoch 8/150\n",
      "19997/19997 [==============================] - 224s - loss: 0.1324 - f1score: 0.2763   \n",
      "Epoch 9/150\n",
      "19997/19997 [==============================] - 228s - loss: 0.1446 - f1score: 0.2282   \n",
      "Epoch 10/150\n",
      "19997/19997 [==============================] - 224s - loss: 0.1306 - f1score: 0.2858   \n",
      "Epoch 11/150\n",
      "19997/19997 [==============================] - 226s - loss: 0.1285 - f1score: 0.2893   \n",
      "Epoch 12/150\n",
      "19997/19997 [==============================] - 235s - loss: 0.1354 - f1score: 0.2619   \n",
      "Epoch 13/150\n",
      "19997/19997 [==============================] - 235s - loss: 0.1253 - f1score: 0.3005   \n",
      "Epoch 14/150\n",
      "19997/19997 [==============================] - 239s - loss: 0.1212 - f1score: 0.3053   \n",
      "Epoch 15/150\n",
      "19997/19997 [==============================] - 236s - loss: 0.1215 - f1score: 0.3181   \n",
      "Epoch 16/150\n",
      "19997/19997 [==============================] - 233s - loss: 0.1170 - f1score: 0.3311   \n",
      "Epoch 17/150\n",
      "19997/19997 [==============================] - 243s - loss: 0.1189 - f1score: 0.3347   \n",
      "Epoch 18/150\n",
      "19997/19997 [==============================] - 239s - loss: 0.1135 - f1score: 0.3430   \n",
      "Epoch 19/150\n",
      "19997/19997 [==============================] - 240s - loss: 0.1113 - f1score: 0.3592   \n",
      "Epoch 20/150\n",
      "19997/19997 [==============================] - 238s - loss: 0.1092 - f1score: 0.3632   \n",
      "Epoch 21/150\n",
      "19997/19997 [==============================] - 238s - loss: 0.1065 - f1score: 0.3710   \n",
      "Epoch 22/150\n",
      "19997/19997 [==============================] - 239s - loss: 0.1054 - f1score: 0.3858   \n",
      "Epoch 23/150\n",
      "19997/19997 [==============================] - 240s - loss: 0.1035 - f1score: 0.3940   \n",
      "Epoch 24/150\n",
      "19997/19997 [==============================] - 236s - loss: 0.1031 - f1score: 0.3802   \n",
      "Epoch 25/150\n",
      "19997/19997 [==============================] - 237s - loss: 0.0998 - f1score: 0.4051   \n",
      "Epoch 26/150\n",
      "19997/19997 [==============================] - 238s - loss: 0.0999 - f1score: 0.4048   \n",
      "Epoch 27/150\n",
      "19997/19997 [==============================] - 247s - loss: 0.0977 - f1score: 0.4105   \n",
      "Epoch 28/150\n",
      "19997/19997 [==============================] - 246s - loss: 0.0946 - f1score: 0.4191   \n",
      "Epoch 29/150\n",
      "19997/19997 [==============================] - 245s - loss: 0.0935 - f1score: 0.4285   \n",
      "Epoch 30/150\n",
      "19997/19997 [==============================] - 250s - loss: 0.0911 - f1score: 0.4394   \n",
      "Epoch 31/150\n",
      "19997/19997 [==============================] - 251s - loss: 0.0911 - f1score: 0.4449   \n",
      "Epoch 32/150\n",
      "19997/19997 [==============================] - 252s - loss: 0.0895 - f1score: 0.4454   \n",
      "Epoch 33/150\n",
      "19997/19997 [==============================] - 246s - loss: 0.0894 - f1score: 0.4539   \n",
      "Epoch 34/150\n",
      "19997/19997 [==============================] - 234s - loss: 0.0877 - f1score: 0.4562   \n",
      "Epoch 35/150\n",
      "19997/19997 [==============================] - 249s - loss: 0.0855 - f1score: 0.4694   \n",
      "Epoch 36/150\n",
      "19997/19997 [==============================] - 252s - loss: 0.0850 - f1score: 0.4562   \n",
      "Epoch 37/150\n",
      "19997/19997 [==============================] - 249s - loss: 0.0828 - f1score: 0.4861   \n",
      "Epoch 38/150\n",
      "19997/19997 [==============================] - 246s - loss: 0.0818 - f1score: 0.4853   \n",
      "Epoch 39/150\n",
      "19997/19997 [==============================] - 248s - loss: 0.0811 - f1score: 0.5010   \n",
      "Epoch 40/150\n",
      "19997/19997 [==============================] - 250s - loss: 0.0791 - f1score: 0.5024   \n",
      "Epoch 41/150\n",
      "19997/19997 [==============================] - 255s - loss: 0.0812 - f1score: 0.4921   \n",
      "Epoch 42/150\n",
      "19997/19997 [==============================] - 251s - loss: 0.0775 - f1score: 0.5037   \n",
      "Epoch 43/150\n",
      "19997/19997 [==============================] - 258s - loss: 0.0770 - f1score: 0.5055   \n",
      "Epoch 44/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0752 - f1score: 0.5199   \n",
      "Epoch 45/150\n",
      "19997/19997 [==============================] - 257s - loss: 0.0744 - f1score: 0.5214   \n",
      "Epoch 46/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0734 - f1score: 0.5284   \n",
      "Epoch 47/150\n",
      "19997/19997 [==============================] - 255s - loss: 0.0724 - f1score: 0.5418   \n",
      "Epoch 48/150\n",
      "19997/19997 [==============================] - 255s - loss: 0.0732 - f1score: 0.5186   \n",
      "Epoch 49/150\n",
      "19997/19997 [==============================] - 257s - loss: 0.0687 - f1score: 0.5667   \n",
      "Epoch 50/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0690 - f1score: 0.5493   \n",
      "Epoch 51/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0694 - f1score: 0.5577   \n",
      "Epoch 52/150\n",
      "19997/19997 [==============================] - 265s - loss: 0.0656 - f1score: 0.5751   \n",
      "Epoch 53/150\n",
      "19997/19997 [==============================] - 263s - loss: 0.0661 - f1score: 0.5630   \n",
      "Epoch 54/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0642 - f1score: 0.5765   \n",
      "Epoch 55/150\n",
      "19997/19997 [==============================] - 254s - loss: 0.0639 - f1score: 0.5745   \n",
      "Epoch 56/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0658 - f1score: 0.5716   \n",
      "Epoch 57/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0625 - f1score: 0.5863   \n",
      "Epoch 58/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0610 - f1score: 0.5904   \n",
      "Epoch 59/150\n",
      "19997/19997 [==============================] - 265s - loss: 0.0619 - f1score: 0.5944   \n",
      "Epoch 60/150\n",
      "19997/19997 [==============================] - 271s - loss: 0.0618 - f1score: 0.5865   \n",
      "Epoch 61/150\n",
      "19997/19997 [==============================] - 262s - loss: 0.0587 - f1score: 0.6345   \n",
      "Epoch 62/150\n",
      "19997/19997 [==============================] - 256s - loss: 0.0607 - f1score: 0.6028   \n",
      "Epoch 63/150\n",
      "19997/19997 [==============================] - 261s - loss: 0.0588 - f1score: 0.6147   \n",
      "Epoch 64/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0574 - f1score: 0.6280   \n",
      "Epoch 65/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0579 - f1score: 0.6293   \n",
      "Epoch 66/150\n",
      "19997/19997 [==============================] - 257s - loss: 0.0552 - f1score: 0.6258   \n",
      "Epoch 67/150\n",
      "19997/19997 [==============================] - 261s - loss: 0.0545 - f1score: 0.6387   \n",
      "Epoch 68/150\n",
      "19997/19997 [==============================] - 272s - loss: 0.0563 - f1score: 0.6333   \n",
      "Epoch 69/150\n",
      "19997/19997 [==============================] - 267s - loss: 0.0531 - f1score: 0.6448   \n",
      "Epoch 70/150\n",
      "19997/19997 [==============================] - 272s - loss: 0.0545 - f1score: 0.6225   \n",
      "Epoch 71/150\n",
      "19997/19997 [==============================] - 271s - loss: 0.0527 - f1score: 0.6450   \n",
      "Epoch 72/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997/19997 [==============================] - 268s - loss: 0.0531 - f1score: 0.6444   \n",
      "Epoch 73/150\n",
      "19997/19997 [==============================] - 271s - loss: 0.0502 - f1score: 0.6650   \n",
      "Epoch 74/150\n",
      "19997/19997 [==============================] - 269s - loss: 0.0522 - f1score: 0.6415   \n",
      "Epoch 75/150\n",
      "19997/19997 [==============================] - 272s - loss: 0.0496 - f1score: 0.6644   \n",
      "Epoch 76/150\n",
      "19997/19997 [==============================] - 278s - loss: 0.0502 - f1score: 0.6608   \n",
      "Epoch 77/150\n",
      "19997/19997 [==============================] - 278s - loss: 0.0495 - f1score: 0.6802   \n",
      "Epoch 78/150\n",
      "19997/19997 [==============================] - 288s - loss: 0.0488 - f1score: 0.6787   \n",
      "Epoch 79/150\n",
      "19997/19997 [==============================] - 278s - loss: 0.0459 - f1score: 0.6885   \n",
      "Epoch 80/150\n",
      "19997/19997 [==============================] - 279s - loss: 0.0492 - f1score: 0.6654   \n",
      "Epoch 81/150\n",
      "19997/19997 [==============================] - 281s - loss: 0.0479 - f1score: 0.6644   \n",
      "Epoch 82/150\n",
      "19997/19997 [==============================] - 285s - loss: 0.0487 - f1score: 0.6742   \n",
      "Epoch 83/150\n",
      "19997/19997 [==============================] - 273s - loss: 0.0447 - f1score: 0.6984   \n",
      "Epoch 84/150\n",
      "19997/19997 [==============================] - 272s - loss: 0.0448 - f1score: 0.6952   \n",
      "Epoch 85/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0463 - f1score: 0.6793   \n",
      "Epoch 86/150\n",
      "19997/19997 [==============================] - 267s - loss: 0.0454 - f1score: 0.6982   \n",
      "Epoch 87/150\n",
      "19997/19997 [==============================] - 270s - loss: 0.0427 - f1score: 0.7009   \n",
      "Epoch 88/150\n",
      "19997/19997 [==============================] - 282s - loss: 0.0449 - f1score: 0.6967   \n",
      "Epoch 89/150\n",
      "19997/19997 [==============================] - 273s - loss: 0.0455 - f1score: 0.7007   \n",
      "Epoch 90/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0419 - f1score: 0.7136   \n",
      "Epoch 91/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0435 - f1score: 0.7011   \n",
      "Epoch 92/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0420 - f1score: 0.7139   \n",
      "Epoch 93/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0386 - f1score: 0.7302   \n",
      "Epoch 94/150\n",
      "19997/19997 [==============================] - 267s - loss: 0.0414 - f1score: 0.7056   \n",
      "Epoch 95/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0424 - f1score: 0.7181   \n",
      "Epoch 96/150\n",
      "19997/19997 [==============================] - 267s - loss: 0.0440 - f1score: 0.7038   \n",
      "Epoch 97/150\n",
      "19997/19997 [==============================] - 266s - loss: 0.0410 - f1score: 0.7015   \n",
      "Epoch 98/150\n",
      "19997/19997 [==============================] - 268s - loss: 0.0401 - f1score: 0.7175   \n",
      "Epoch 99/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0388 - f1score: 0.7378   \n",
      "Epoch 100/150\n",
      "19997/19997 [==============================] - 262s - loss: 0.0396 - f1score: 0.7299   \n",
      "Epoch 101/150\n",
      "19997/19997 [==============================] - 262s - loss: 0.0394 - f1score: 0.7275   \n",
      "Epoch 102/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0380 - f1score: 0.7464   \n",
      "Epoch 103/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0390 - f1score: 0.7295   \n",
      "Epoch 104/150\n",
      "19997/19997 [==============================] - 258s - loss: 0.0362 - f1score: 0.7530   \n",
      "Epoch 105/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0392 - f1score: 0.7152   \n",
      "Epoch 106/150\n",
      "19997/19997 [==============================] - 268s - loss: 0.0484 - f1score: 0.7049   \n",
      "Epoch 107/150\n",
      "19997/19997 [==============================] - 291s - loss: 0.0533 - f1score: 0.6587   \n",
      "Epoch 108/150\n",
      "19997/19997 [==============================] - 288s - loss: 0.0459 - f1score: 0.6813   \n",
      "Epoch 109/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0381 - f1score: 0.7227   \n",
      "Epoch 110/150\n",
      "19997/19997 [==============================] - 268s - loss: 0.0403 - f1score: 0.7260   \n",
      "Epoch 111/150\n",
      "19997/19997 [==============================] - 265s - loss: 0.0376 - f1score: 0.7332   \n",
      "Epoch 112/150\n",
      "19997/19997 [==============================] - 259s - loss: 0.0366 - f1score: 0.7351   \n",
      "Epoch 113/150\n",
      "19997/19997 [==============================] - 257s - loss: 0.0360 - f1score: 0.7568   \n",
      "Epoch 114/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0358 - f1score: 0.7527   \n",
      "Epoch 115/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0364 - f1score: 0.7486   \n",
      "Epoch 116/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0343 - f1score: 0.7496   \n",
      "Epoch 117/150\n",
      "19997/19997 [==============================] - 258s - loss: 0.0344 - f1score: 0.7604   \n",
      "Epoch 118/150\n",
      "19997/19997 [==============================] - 263s - loss: 0.0340 - f1score: 0.7571   \n",
      "Epoch 119/150\n",
      "19997/19997 [==============================] - 262s - loss: 0.0375 - f1score: 0.7418   \n",
      "Epoch 120/150\n",
      "19997/19997 [==============================] - 262s - loss: 0.0348 - f1score: 0.7438   \n",
      "Epoch 121/150\n",
      "19997/19997 [==============================] - 255s - loss: 0.0325 - f1score: 0.7602   \n",
      "Epoch 122/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0330 - f1score: 0.7684   \n",
      "Epoch 123/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0352 - f1score: 0.7353   \n",
      "Epoch 124/150\n",
      "19997/19997 [==============================] - 260s - loss: 0.0353 - f1score: 0.7698   \n",
      "Epoch 125/150\n",
      "19997/19997 [==============================] - 268s - loss: 0.0346 - f1score: 0.7497   \n",
      "Epoch 126/150\n",
      "19997/19997 [==============================] - 290s - loss: 0.0323 - f1score: 0.7634   \n",
      "Epoch 127/150\n",
      "19997/19997 [==============================] - 280s - loss: 0.0341 - f1score: 0.7686   \n",
      "Epoch 128/150\n",
      "19997/19997 [==============================] - 295s - loss: 0.0335 - f1score: 0.7701   \n",
      "Epoch 129/150\n",
      "19997/19997 [==============================] - 286s - loss: 0.0332 - f1score: 0.7633   \n",
      "Epoch 130/150\n",
      "19997/19997 [==============================] - 291s - loss: 0.0314 - f1score: 0.7749   \n",
      "Epoch 131/150\n",
      "19997/19997 [==============================] - 295s - loss: 0.0343 - f1score: 0.7554   \n",
      "Epoch 132/150\n",
      "19997/19997 [==============================] - 306s - loss: 0.0324 - f1score: 0.7584   \n",
      "Epoch 133/150\n",
      "19997/19997 [==============================] - 287s - loss: 0.0303 - f1score: 0.7776   \n",
      "Epoch 134/150\n",
      "19997/19997 [==============================] - 276s - loss: 0.0322 - f1score: 0.7772   \n",
      "Epoch 135/150\n",
      "19997/19997 [==============================] - 271s - loss: 0.0311 - f1score: 0.7598   \n",
      "Epoch 136/150\n",
      "19997/19997 [==============================] - 265s - loss: 0.0292 - f1score: 0.7770   \n",
      "Epoch 137/150\n",
      "19997/19997 [==============================] - 263s - loss: 0.0323 - f1score: 0.7610   \n",
      "Epoch 138/150\n",
      "19997/19997 [==============================] - 269s - loss: 0.0341 - f1score: 0.7549   \n",
      "Epoch 139/150\n",
      "19997/19997 [==============================] - 276s - loss: 0.0323 - f1score: 0.7640   \n",
      "Epoch 140/150\n",
      "19997/19997 [==============================] - 280s - loss: 0.0281 - f1score: 0.7733   \n",
      "Epoch 141/150\n",
      "19997/19997 [==============================] - 291s - loss: 0.0293 - f1score: 0.7872   \n",
      "Epoch 142/150\n",
      "19997/19997 [==============================] - 280s - loss: 0.0303 - f1score: 0.7805   \n",
      "Epoch 143/150\n",
      "19997/19997 [==============================] - 264s - loss: 0.0293 - f1score: 0.7668   \n",
      "Epoch 144/150\n",
      "19997/19997 [==============================] - 265s - loss: 0.0282 - f1score: 0.7678   \n",
      "Epoch 145/150\n",
      "19997/19997 [==============================] - 269s - loss: 0.0275 - f1score: 0.7984   \n",
      "Epoch 146/150\n",
      "19997/19997 [==============================] - 270s - loss: 0.0309 - f1score: 0.7821   \n",
      "Epoch 147/150\n",
      "19997/19997 [==============================] - 297s - loss: 0.0275 - f1score: 0.7834   \n",
      "Epoch 148/150\n",
      "19997/19997 [==============================] - 286s - loss: 0.0293 - f1score: 0.7805   \n",
      "Epoch 149/150\n",
      "19997/19997 [==============================] - 281s - loss: 0.0263 - f1score: 0.7725   \n",
      "Epoch 150/150\n",
      "19997/19997 [==============================] - 294s - loss: 0.0289 - f1score: 0.7768   \n",
      "F1 Score: 52.66%\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras import backend as K\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "from utils import TextLoader\n",
    "\n",
    "utils_dir = 'utils'\n",
    "data_path = 'data/data_2w_arti_clean.csv'\n",
    "test_data_path = 'data/data_test_arti_1_clean.csv'\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 500\n",
    "batch_size = 64\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 150\n",
    "\n",
    "def f1score(y_true, y_pred):\n",
    "    num_tp = K.sum(y_true*y_pred)\n",
    "    num_fn = K.sum(y_true*(1.0-y_pred))\n",
    "    num_fp = K.sum((1.0-y_true)*y_pred)\n",
    "    num_tn = K.sum((1.0-y_true)*(1.0-y_pred))\n",
    "    #print num_tp, num_fn, num_fp, num_tn\n",
    "    f1 = 2.0*num_tp/(2.0*num_tp+num_fn+num_fp)\n",
    "    return f1\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "data_loader = TextLoader(True, utils_dir, data_path, batch_size, 20, None, None)\n",
    "data_test_loader = TextLoader(True, utils_dir, test_data_path, batch_size, 20, None, None)\n",
    "\n",
    "x_train = data_loader.tensor_xa\n",
    "y_train = data_loader.tensor_y\n",
    "\n",
    "x_test = data_test_loader.tensor_xa\n",
    "y_test = data_test_loader.tensor_y\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embedding_vecor_length, input_length=maxlen))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=[f1score])\n",
    "\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "# validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"F1 Score: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
